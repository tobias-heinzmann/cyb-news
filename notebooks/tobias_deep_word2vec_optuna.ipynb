{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bca0d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import string\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.config.experimental import list_physical_devices, set_memory_growth\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Dropout, SpatialDropout1D, SimpleRNN\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import optuna\n",
    "\n",
    "from cybnews.data import get_data, welf_join_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4da8ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "print(\"Num GPUs Available: \", len(list_physical_devices('GPU')))\n",
    "\n",
    "gpus = list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce3fac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WELF dataset\n",
    "#\n",
    "#\n",
    "DATA_PATH = '/home/tober/devel/lewagon/project/cyb-news/data'\n",
    "data = welf_join_text(get_data(f'{DATA_PATH}/WELFake_Dataset.csv'))[['all_text', 'label']]\n",
    "\n",
    "#data = data.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53da12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    \n",
    "    for x in string.punctuation:\n",
    "        sentence = sentence.replace(x, '')\n",
    "\n",
    "    for x in ['’', '“', '”', '-', '\"' ]:\n",
    "        sentence = sentence.replace(x, '')\n",
    "    return sentence\n",
    "\n",
    "data[\"all_text_cleaned\"] = data[\"all_text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3348a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length: 5000\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 5000  # Maximum sequence length\n",
    "print(f'max_seq_length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5cf41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.all_text_cleaned\n",
    "y = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(X)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_tokenized, maxlen=max_seq_length, dtype='float32', padding='pre', value=0)\n",
    "X_test_padded = pad_sequences(X_test_tokenized, maxlen=max_seq_length, dtype='float32', padding='pre', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7823f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50493, 5000)\n",
      "(50493,)\n",
      "law enforcement on high alert following threats against cops and whites on by blacklivesmatter and fyf terrorists video no comment is expected from barack obama members of the fyf or fukyoflag and blacklivesmatter movements called for the lynching and hanging of white people and cops they encouraged others on a radio show tuesday night to  turn the tide  and kill white people and cops to send a message about the killing of black people in americaone of the fyoflag organizers is called  sunshine  she has a radio blog show hosted from texas called  sunshine s fing opinion radio show a snapshot of her fyf lolatwhitefear twitter page at  pm shows that she was urging supporters to  call now fyf tonight we continue to dismantle the illusion of white below is a snapshot twitter radio call invite   fyfthe radio show aired at  pm eastern standard timeduring the show callers clearly call for  lynching  and  killing  of white peoplea  minute clip from the radio show can be heard here it was provided to breitbart texas by someone who would like to be referred to as  hannibal  he has already received death threats as a result of interrupting fyf conference callsan unidentified black man said  when those mother fkers are by themselves that s when when we should start fing them up like they do us when a bunch of them niers takin  one of us out that s how we should roll up  he said  cause we already roll up in gangs anyway there should be six or seven black mother fckers see that white person and then lynch their ass let s turn the tables they conspired that if  cops started losing people  then  there will be a state of emergency he speculated that one of two things would happen  a bigass r s war  or  niers they are going to start backin  up we are already getting killed out here so what the fk we got to lose sunshine could be heard saying  yep that s true that s so fking true he said  we need to turn the tables on them our kids are getting shot out here somebody needs to become a sacrifice on their sidehe said  everybody ain t down for that st or whatever but like i say everybody has a different position of war  he continued  because they don t give a fk anyway  he said again  we might as well utilized them for that st and turn the tables on these ners he said that way  we can start lookin  like we ain t havin  that many casualties and there can be more causalities on their side instead of ours they are out their killing black people black lives don t matter that s what those mother fkers   so we got to make it matter to them find a mother fker that is alone snap his ass and then fin hang him from a damn tree take a picture of it and then send it to the mother fkers we  just need one example  and  then people will start watchin   this will turn the tables on st he said he said this will start  a trickledown effect  he said that when one white person is hung and then they are just  flathanging  that will start the  trickledown effect  he continued  black people are good at starting trends he said that was how  to get the upperhand another black man spoke up saying they needed to kill  cops that are killing us the first black male said  that will be the best method right there breitbart texas previously reported how sunshine was upset when  racist white people  infiltrated and disrupted one of her conference calls she subsequently released the phone number of one of the infiltrators the veteran immediately started receiving threatening callsone of the fyoflag movement supporters allegedly told a veteran who infiltrated their publicly posted conference call  we are going to rape and gut your pregnant wife and your fing piece of sht unborn creature will be hung from a tree breitbart texas previously encountered sunshine at a sandra bland protest at the waller county jail in texas where she said all white people should be killed she told journalists and photographers  you see this nappyass hair on my head   that means i am one of those more militant negroes  she said she was at the protest because  these redneck motherfkers murdered sandra bland because she had nappy hair like me fyf black radicals say they will be holding the  imperial powers  that are actually responsible for the terrorist attacks on september th accountable on that day as reported by breitbart texas there are several websites and twitter handles for the movement palmetto star  describes himself as one of the head organizers he said in a youtube video that supporters will be burning their symbols of  the illusion of their superiority  their  false white supremacy  like the american flag the british flag police uniforms and ku klux klan hoodssierra mcgrone or  nocturnus libertus  posted  you too can help a young afrikan clean their a with the rag of oppression  she posted two photos one that appears to be herself and a photo of a black man wiping their naked butts with the american flagfor entire story breitbart news\n",
      "[130, 858, 6025, 59, 18, 1741, 4381, 175, 1283, 79, 45473, 510, 51, 18, 10, 441, 1475, 5, 175, 3, 79, 45473, 132, 6313, 3667, 688, 7293, 9403, 542, 465, 14, 3011, 17, 1, 510, 103483, 1134, 52, 16, 45474, 34952, 2129, 2, 5, 175, 3, 1, 51, 72362, 34953, 4, 135, 30545, 196, 9403, 20, 3068, 129, 5, 1209, 3, 1, 5714, 8059, 26, 50959, 54510, 6, 45474, 34952, 45475, 91, 689, 18, 979, 597, 9403, 8, 1, 1193, 3, 7377, 9403, 31, 2055, 18, 20, 547, 288, 103484, 498, 83, 3389, 10894, 4, 1431, 5670, 2498, 6602, 1, 1024, 31, 14, 1387, 10, 175, 735, 36, 89, 12, 14, 4261, 813, 894, 2, 9026, 469, 26, 18, 2346, 26, 1, 105, 88, 69, 157, 112, 12, 14, 6, 197, 59, 1, 4147, 175, 14, 471, 7, 2136, 1, 220, 324, 2, 9026, 469, 93, 2286, 18, 380, 1, 130, 14, 3365, 1, 130, 14, 65, 3365, 57, 11, 14, 1875, 7, 83, 20, 548, 240, 1799, 3700, 234, 3087, 1174, 2, 455, 47, 2, 976, 93, 675, 10643, 6, 1, 5226, 10934, 1898, 224, 52, 14, 1, 13188, 3, 1, 1120, 214, 508, 397, 738, 110, 1035, 3171, 1, 99, 877, 6, 83, 20, 324, 2, 3392, 32094, 5, 221, 10, 254, 510, 2769, 561, 27, 105, 88, 1519, 2410, 4661, 344, 104, 186, 7, 1, 51, 880, 90, 12028, 469, 93, 1, 130, 1, 51, 6, 63, 141, 946, 42, 3029, 23, 379, 4103, 42, 3115, 469, 53, 476, 1138, 1, 3812, 103485, 1, 510, 1134, 157509, 5661, 93, 1, 13425, 175, 772, 2, 978, 441, 157510, 1347, 1799, 37273, 1480, 54, 180, 2, 7395, 1, 1134, 42, 21, 7857, 6, 44, 649, 32, 422, 678, 10, 215, 54511, 479, 3, 230, 2, 4705, 28, 35, 43, 740, 2, 21, 5, 1481, 10, 226, 1481, 11, 42, 35, 12028, 11, 1, 613, 1013, 1475, 10, 157511, 88, 317, 3307, 2858, 65, 4599, 1, 1134, 469, 1120, 8591, 2, 5165, 1, 373, 318, 21, 1854, 28, 157512, 3649, 743, 3, 1, 373, 14392, 2, 68, 1079, 48, 381, 10, 157513, 2942, 6458, 7103, 64, 1, 130, 4641, 14580, 13, 28, 50, 72363, 8, 6913, 469, 13, 5, 6313, 6038, 176, 207, 130, 7, 1912, 5986, 5, 72364, 1259, 10, 139, 468, 157514, 612, 2942, 6458, 16185, 597, 5, 18, 191, 283, 16, 12, 880, 90, 795, 18, 20, 1134, 15, 6913, 469, 93, 1, 130, 1057, 24, 295, 32, 145, 7, 63, 50, 40, 9841, 7, 15, 5, 635, 32, 624, 30, 369, 90, 32, 80, 145, 7, 18, 8, 4496, 64, 6, 5, 141, 10, 3119, 5671, 7, 12, 23, 5, 168, 2, 1258, 2, 1672, 149, 996, 1784, 17609, 16, 10, 2051, 20, 28, 186, 12, 16, 3119, 101, 636, 25, 24, 478, 136, 1420, 2, 1, 51, 31, 2503, 5, 304, 130, 1907, 2, 931, 1143, 2, 1, 113, 47, 18, 8, 1, 106, 5039, 51, 4, 1, 106, 41644, 51, 34, 784, 353, 6, 121, 4388, 12, 27996, 84368, 1006, 332, 18, 1076, 4, 5, 4728, 1, 51, 14, 65, 1, 1281, 3, 2536, 14047, 104, 273, 57, 12, 818, 1, 7470, 3474, 1134, 36, 9027, 13, 98, 667, 217, 11, 14, 5558, 1, 5005, 3, 1, 72365, 2444, 19, 218, 3, 304, 130, 6, 5, 1134, 10, 619, 121, 218, 3, 304, 130, 8, 24, 165, 11, 20, 1526, 162, 165, 127, 173, 293, 276, 12, 381, 157515, 2, 510, 15552, 5, 175, 7, 20, 45, 204, 9, 100, 1283, 5, 841, 157516, 17, 135, 2601, 18, 8, 5, 270, 4581, 27, 3009, 469, 53, 35, 43, 66, 1179, 36, 469, 198, 822, 90, 35, 80, 102, 1207, 10, 10894, 1193, 3, 5, 96, 78, 6952, 4, 527, 5128, 31, 14, 1387, 10, 3135, 1622, 2, 9026, 469, 26, 18, 2346, 114, 12, 92, 661, 68, 51]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_padded.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(X_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e5c51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 319480\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.index_word) + 1  # also num of features\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db6641",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "265cfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 400\n",
    "window_size = 4\n",
    "\n",
    "word_sentences = [word_tokenize(sentence) for sentence in X]\n",
    "word2vec_model = Word2Vec(sentences=word_sentences, vector_size=vector_size, window=window_size, min_count=1, workers=4)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, word2vec_model.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf09b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    second_gru_layer = trial.suggest_int('second_gru_layer', 0, 1)\n",
    "    deep_layers = trial.suggest_int(\"deep_layer\", 0, 1)\n",
    "    gru_units = trial.suggest_int(\"gru_units\", 50, 500)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=word2vec_model.vector_size, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "    model.add(\n",
    "        SpatialDropout1D(\n",
    "            trial.suggest_float('spatial_dropout_rate', 0.1, 0.4)\n",
    "        )\n",
    "    )\n",
    "    if bool(second_gru_layer):\n",
    "        model.add(\n",
    "            GRU(\n",
    "                gru_units,\n",
    "                return_sequences=True\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            GRU(trial.suggest_int('second_gru_units', 50, 200))\n",
    "        )\n",
    "    else:\n",
    "        model.add(\n",
    "            GRU(gru_units)\n",
    "        )\n",
    "\n",
    "    for i in range(deep_layers):\n",
    "        num_hidden = trial.suggest_int(\"num_neurons_hidden{}\".format(i), 4, 128, log=True)\n",
    "        model.add(\n",
    "            Dense(\n",
    "                num_hidden,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=l2(trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)),\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    #learning_rate = trial.suggest_loguniform('learning_rate', 0.00001, 0.001)\n",
    "    #batch_size = trial.suggest_int('batch_size', 8, 16)\n",
    "    #epochs = trial.suggest_int('epochs', 5, 10)\n",
    "\n",
    "    model = create_model(trial)\n",
    "    optimizer = Adam()#learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_padded,\n",
    "        y_train,\n",
    "        epochs=3,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    loss = min(history.history['loss'])\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    print(f'loss: {str(loss)}, val_loss: {str(val_loss)}, train_accuracy: {str(train_accuracy)}, val_accuracy: {str(val_accuracy)}')\n",
    "    \n",
    "    return max(val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "474b11c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-23 11:50:22,816] A new study created in RDB with name: 2024-07-23_11:50:22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9a4bc36e6e4280af0a3a39631e7f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1263/1263 [==============================] - 588s 464ms/step - loss: 0.1130 - accuracy: 0.9548 - val_loss: 0.0473 - val_accuracy: 0.9819\n",
      "Epoch 2/3\n",
      " 153/1263 [==>...........................] - ETA: 7:49 - loss: 0.0382 - accuracy: 0.9855[W 2024-07-23 12:01:18,071] Trial 0 failed with parameters: {'second_gru_layer': 1, 'deep_layer': 0, 'gru_units': 472, 'spatial_dropout_rate': 0.2187475676378805, 'second_gru_units': 74} because of the following error: InternalError().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tober/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_10969/3773704726.py\", line 57, in objective\n",
      "    history = model.fit(\n",
      "  File \"/home/tober/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/tober/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InternalError: Graph execution error:\n",
      "\n",
      "Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 400, 472, 1, 5000, 32, 0] \n",
      "\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n",
      "\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_29317]\n",
      "[W 2024-07-23 12:01:18,073] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 400, 472, 1, 5000, 32, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_29317]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m storage \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mstorages\u001b[38;5;241m.\u001b[39mRDBStorage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///optuna_study.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, storage\u001b[38;5;241m=\u001b[39mstorage, study_name\u001b[38;5;241m=\u001b[39mstudy_name)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [55], line 57\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     52\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     53\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     54\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     67\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 400, 472, 1, 5000, 32, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall_1]] [Op:__inference_train_function_29317]"
     ]
    }
   ],
   "source": [
    "study_name = datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "storage = optuna.storages.RDBStorage('sqlite:///optuna_study.db')\n",
    "study = optuna.create_study(direction='maximize', storage=storage, study_name=study_name)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#study = optuna.load_study(study_name='cybnews', storage='sqlite:///optuna_study.db')\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial value (validation loss): {best_trial.value}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
